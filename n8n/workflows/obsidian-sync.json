{
  "meta": {
    "instanceId": "obsidian-sync-template-workflow"
  },
  "nodes": [
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "field": "cronExpression",
              "expression": "0 */30 * * * *"
            }
          ]
        }
      },
      "name": "Cron Trigger",
      "type": "n8n-nodes-base.cron",
      "typeVersion": 1,
      "position": [200, 300],
      "id": "cronTrigger1",
      "notes": "Configurable schedule trigger - default every 30 minutes\nModify cronExpression in workflow settings:\n- Every hour: 0 0 * * * *\n- Every day at 2 AM: 0 0 2 * * *\n- Every 15 minutes: 0 */15 * * * *"
    },
    {
      "parameters": {},
      "name": "Manual Trigger",
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [200, 200],
      "id": "manualTrigger1",
      "notes": "Manual trigger for testing and on-demand sync"
    },
    {
      "parameters": {
        "functionCode": "// Tenant-agnostic configuration - all values configurable via environment variables\nconst config = {\n  obsidianVaultPath: $env.OBSIDIAN_VAULT_PATH || '/default-vault-path',\n  tenantId: $env.TENANT_ID || 'default-tenant',\n  batchSize: parseInt($env.BATCH_SIZE) || 10,\n  maxFiles: parseInt($env.MAX_FILES_PER_RUN) || 100,\n  fileExtensions: ($env.FILE_EXTENSIONS || '.md,.markdown').split(','),\n  excludePaths: ($env.EXCLUDE_PATHS || '.obsidian,.trash,templates').split(','),\n  incrementalSync: ($env.INCREMENTAL_SYNC || 'true') === 'true',\n  acl: ($env.DEFAULT_ACL || 'public').split(','),\n  source: $env.SOURCE_NAME || 'obsidian',\n  lang: $env.DEFAULT_LANG || 'en',\n  useIdempotency: ($env.USE_IDEMPOTENCY || 'true') === 'true',\n  enableTombstones: ($env.ENABLE_TOMBSTONES || 'true') === 'true',\n  credentialName: $env.CREDENTIAL_NAME || 'defaultIngestToken'\n};\n\n// Validate required configuration\nconst requiredEnvVars = ['OBSIDIAN_VAULT_PATH', 'TENANT_ID'];\nconst missingVars = requiredEnvVars.filter(varName => !$env[varName]);\n\nif (missingVars.length > 0) {\n  throw new Error(`Missing required environment variables: ${missingVars.join(', ')}`);\n}\n\n// Get last run state from workflow static data\nconst staticData = $workflow.staticData || {};\nconst lastRunTime = staticData.lastRunTime || null;\nconst currentRunTime = new Date().toISOString();\nconst fileHashes = config.useIdempotency ? (staticData.fileHashes || {}) : {};\nconst previousFiles = config.enableTombstones ? (staticData.previousFiles || []) : [];\n\nconsole.log(`${config.tenantId} Obsidian Sync - Starting run at ${currentRunTime}`);\nconsole.log(`Configuration:`);\nconsole.log(`- Vault: ${config.obsidianVaultPath}`);\nconsole.log(`- Tenant: ${config.tenantId}`);\nconsole.log(`- Batch size: ${config.batchSize}`);\nconsole.log(`- Incremental: ${config.incrementalSync}`);\nconsole.log(`- Idempotency: ${config.useIdempotency}`);\nconsole.log(`- Tombstones: ${config.enableTombstones}`);\nconsole.log(`- Last run: ${lastRunTime || 'Never'}`);\n\nreturn [{\n  config,\n  lastRunTime,\n  currentRunTime,\n  fileHashes,\n  previousFiles\n}];"
      },
      "name": "Initialize Configuration",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [400, 250],
      "id": "initConfig1",
      "notes": "Initialize tenant-agnostic configuration via environment variables\nRequired: OBSIDIAN_VAULT_PATH, TENANT_ID\nOptional: BATCH_SIZE, MAX_FILES_PER_RUN, FILE_EXTENSIONS, EXCLUDE_PATHS, etc.\nManages state tracking for idempotency and tombstone detection"
    },
    {
      "parameters": {
        "functionCode": "// Cross-platform file discovery with metadata\nconst config = $json.config;\nconst isWindows = process.platform === 'win32';\n\nlet command;\nif (isWindows) {\n  // Windows PowerShell command\n  command = `powershell.exe -Command \"Get-ChildItem '${config.obsidianVaultPath}' -Recurse ${config.fileExtensions.map(ext => `-Filter '*${ext}'`).join(' -or ')} | Select-Object FullName, LastWriteTime | ForEach-Object { Write-Output \\\"$($_.FullName)|$($_.LastWriteTime.ToString('yyyy-MM-ddTHH:mm:ss.fffZ'))\\\" }\"`;\n} else {\n  // Unix/Linux find command\n  const findExpr = config.fileExtensions.map(ext => `-name '*${ext}'`).join(' -o ');\n  command = `find \"${config.obsidianVaultPath}\" -type f \\( ${findExpr} \\) -printf \"%p|%TY-%Tm-%TdT%TH:%TM:%TS%Tz\\n\"`;\n}\n\nreturn [{ command, config, platform: process.platform }];"
      },
      "name": "Build File Discovery Command",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [600, 250],
      "id": "buildCommand1",
      "notes": "Build cross-platform file discovery command\nSupports Windows PowerShell and Unix find commands\nExtracts file paths with modification timestamps"
    },
    {
      "parameters": {
        "command": "={{ $json.command }}"
      },
      "name": "Find Markdown Files",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [800, 250],
      "id": "findFiles1",
      "notes": "Execute cross-platform file discovery\nFinds markdown files with modification times for idempotency"
    },
    {
      "parameters": {
        "functionCode": "// Parse file list and apply idempotency logic\nconst fileListOutput = $input.first().json.stdout;\nconst config = $input.first().json.config;\nconst fileHashes = $input.first().json.fileHashes;\nconst crypto = require('crypto');\nconst fs = require('fs');\nconst path = require('path');\n\nif (!fileListOutput || fileListOutput.trim() === '') {\n  console.log('No markdown files found in vault');\n  return [];\n}\n\nconst fileLines = fileListOutput.trim().split('\\n').filter(line => line.trim());\nconsole.log(`Found ${fileLines.length} markdown files`);\n\nconst filesToProcess = [];\nconst currentFiles = [];\nconst updatedFileHashes = { ...fileHashes };\nlet skippedCount = 0;\n\nfor (const line of fileLines) {\n  const [filePath, lastWriteTime] = line.split('|');\n  if (!filePath || !lastWriteTime) continue;\n\n  // Filter out excluded paths (cross-platform path separator handling)\n  const normalizedPath = filePath.replace(/\\\\/g, '/');\n  if (config.excludePaths.some(excludePath => normalizedPath.includes(`/${excludePath}/`))) {\n    continue;\n  }\n\n  currentFiles.push(filePath);\n\n  try {\n    // Apply idempotency check if enabled\n    if (config.useIdempotency) {\n      const content = fs.readFileSync(filePath, 'utf8');\n      const fileHash = crypto.createHash('sha256').update(content).digest('hex');\n      const previousHash = fileHashes[filePath];\n      \n      if (previousHash && previousHash === fileHash) {\n        console.log(`SKIP: ${path.basename(filePath)} (unchanged)`);\n        updatedFileHashes[filePath] = fileHash;\n        skippedCount++;\n        continue;\n      }\n\n      console.log(`PROCESS: ${path.basename(filePath)} (${previousHash ? 'modified' : 'new'})`);\n      updatedFileHashes[filePath] = fileHash;\n    } else {\n      console.log(`PROCESS: ${path.basename(filePath)} (idempotency disabled)`);\n    }\n    \n    filesToProcess.push({\n      filePath,\n      lastWriteTime,\n      isNew: !fileHashes[filePath],\n      config\n    });\n    \n  } catch (error) {\n    console.error(`Error processing ${filePath}: ${error.message}`);\n  }\n}\n\nconsole.log(`Processing ${filesToProcess.length} files, skipped ${skippedCount} unchanged files`);\n\n// Store updated state\n$workflow.staticData = {\n  ...$workflow.staticData,\n  currentFiles,\n  updatedFileHashes\n};\n\nreturn filesToProcess;"
      },
      "name": "Apply Idempotency Logic",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [1000, 250],
      "id": "processFiles1",
      "notes": "Apply configurable idempotency logic using file content hashes\nSkips unchanged files when USE_IDEMPOTENCY=true\nCross-platform path handling for exclude patterns"
    },
    {
      "parameters": {
        "functionCode": "// Generic markdown parsing with configurable docId generation\nconst fs = require('fs');\nconst path = require('path');\n\nconst filePath = $json.filePath;\nconst config = $json.config;\n\ntry {\n  const content = fs.readFileSync(filePath, 'utf8');\n  const stats = fs.statSync(filePath);\n  \n  // Parse frontmatter\n  const frontmatterRegex = /^---\\s*\\n([\\s\\S]*?)\\n---\\s*\\n([\\s\\S]*)$/;\n  const match = content.match(frontmatterRegex);\n  \n  let frontmatter = {};\n  let markdownContent = content;\n  \n  if (match) {\n    try {\n      const yamlContent = match[1];\n      const lines = yamlContent.split('\\n');\n      \n      for (const line of lines) {\n        const colonIndex = line.indexOf(':');\n        if (colonIndex > 0) {\n          const key = line.substring(0, colonIndex).trim();\n          const value = line.substring(colonIndex + 1).trim();\n          \n          if (value.startsWith('[') && value.endsWith(']')) {\n            frontmatter[key] = value.slice(1, -1).split(',').map(s => s.trim().replace(/^\"|\"$/g, ''));\n          } else {\n            frontmatter[key] = value.replace(/^\"|\"$/g, '');\n          }\n        }\n      }\n      \n      markdownContent = match[2];\n    } catch (yamlError) {\n      console.log(`Failed to parse frontmatter for ${path.basename(filePath)}:`, yamlError.message);\n    }\n  }\n  \n  // Generate docId as vault-relative forward-slash path\n  const relativePath = path.relative(config.obsidianVaultPath, filePath);\n  const docId = relativePath.replace(/\\.(md|markdown)$/i, '').replace(/\\\\/g, '/');\n  \n  // Extract title\n  const title = frontmatter.title || \n                markdownContent.match(/^#\\s+(.+)$/m)?.[1] || \n                path.basename(filePath, path.extname(filePath));\n  \n  // Parse content into blocks\n  const blocks = [];\n  const sections = markdownContent.split(/\\n(?=#{1,6}\\s)/);\n  \n  for (const section of sections) {\n    const trimmed = section.trim();\n    if (trimmed) {\n      let blockType = 'text';\n      if (trimmed.includes('```')) {\n        blockType = 'code';\n      } else if (trimmed.includes('|') && trimmed.includes('---')) {\n        blockType = 'table';\n      }\n      \n      blocks.push({\n        type: blockType,\n        text: trimmed,\n        html: trimmed\n      });\n    }\n  }\n  \n  // Create NormalizedDoc with configurable metadata\n  const normalizedDoc = {\n    meta: {\n      tenant: config.tenantId,\n      docId: docId,\n      source: config.source,\n      path: relativePath.replace(/\\\\/g, '/'),\n      title: title,\n      lang: frontmatter.lang || config.lang,\n      version: frontmatter.version || '1.0',\n      sha256: '', // Will be calculated by API\n      acl: frontmatter.acl || frontmatter.tags || config.acl,\n      authors: frontmatter.authors || (frontmatter.author ? [frontmatter.author] : []),\n      tags: frontmatter.tags || [],\n      timestamp: new Date().toISOString(),\n      modifiedAt: stats.mtime.toISOString(),\n      deleted: false\n    },\n    blocks: blocks\n  };\n  \n  return [{\n    ...normalizedDoc,\n    _filePath: filePath,\n    _isNew: $json.isNew,\n    _processedAt: new Date().toISOString()\n  }];\n  \n} catch (error) {\n  return [{\n    _error: true,\n    _filePath: filePath,\n    _errorMessage: error.message,\n    _processedAt: new Date().toISOString()\n  }];\n}"
      },
      "name": "Parse Markdown Documents",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [1200, 250],
      "id": "parseMarkdown1",
      "notes": "Generic markdown parsing with configurable metadata\nGenerates docId as vault-relative forward-slash path\nUses tenant-specific configuration for ACL, source, language"
    },
    {
      "parameters": {
        "functionCode": "// Create processing batches with enhanced error handling\nconst validDocs = $input.all().filter(item => !item.json._error);\nconst errorDocs = $input.all().filter(item => item.json._error);\nconst config = validDocs[0]?.json.config || $('Initialize Configuration').item.json.config;\n\nif (errorDocs.length > 0) {\n  console.log(`WARNING: Skipping ${errorDocs.length} files due to errors:`);\n  errorDocs.forEach(doc => {\n    console.log(`- ${doc.json._filePath}: ${doc.json._errorMessage}`);\n  });\n}\n\nif (validDocs.length === 0) {\n  console.log('No valid documents to process');\n  return [];\n}\n\nconsole.log(`Creating batches of ${config.batchSize} documents from ${validDocs.length} valid docs`);\n\nconst batches = [];\nfor (let i = 0; i < validDocs.length; i += config.batchSize) {\n  const batchDocs = validDocs.slice(i, i + config.batchSize);\n  const batch = batchDocs.map(item => {\n    const { _filePath, _isNew, _processedAt, config, ...normalizedDoc } = item.json;\n    return normalizedDoc;\n  });\n  \n  batches.push({\n    batch: batch,\n    batchNumber: Math.floor(i / config.batchSize) + 1,\n    totalBatches: Math.ceil(validDocs.length / config.batchSize),\n    batchSize: batch.length,\n    newDocs: batchDocs.filter(item => item.json._isNew).length,\n    modifiedDocs: batchDocs.filter(item => !item.json._isNew).length,\n    tenantId: config.tenantId\n  });\n}\n\nconsole.log(`Created ${batches.length} batches for ${config.tenantId}`);\nreturn batches;"
      },
      "name": "Create Processing Batches",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [1400, 250],
      "id": "createBatches1",
      "notes": "Create processing batches with configurable batch size\nTracks new vs modified documents for comprehensive audit"
    },
    {
      "parameters": {
        "url": "={{ $env.API_URL }}/ingest/preview",
        "method": "POST",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "x-ingest-token",
              "value": "={{ $credentials[$('Initialize Configuration').item.json.config.credentialName].token }}"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "bodyContentType": "json",
        "jsonBody": "={{ JSON.stringify($json.batch) }}",
        "options": {
          "timeout": "={{ parseInt($env.REQUEST_TIMEOUT) || 45000 }}",
          "retry": {
            "enabled": true,
            "maxTries": "={{ parseInt($env.MAX_RETRIES) || 3 }}",
            "waitBetween": "={{ parseInt($env.RETRY_DELAY) || 2000 }}"
          }
        }
      },
      "name": "Preview Documents",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [1600, 250],
      "id": "previewDocs1",
      "notes": "Preview documents using configurable credentials and timeouts\nUses CREDENTIAL_NAME environment variable for tenant-specific tokens\nConfigurable timeout and retry settings"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "preview-check",
              "leftValue": "={{ $json.wouldPublish }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equal"
              }
            }
          ],
          "combinator": "and"
        }
      },
      "name": "Check Preview Result",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [1800, 250],
      "id": "checkPreview1",
      "notes": "Branch based on PII policy preview result\nTrue: Proceed to publish, False: Log policy block and skip"
    },
    {
      "parameters": {
        "url": "={{ $env.API_URL }}/ingest/publish",
        "method": "POST",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "x-ingest-token",
              "value": "={{ $credentials[$('Initialize Configuration').item.json.config.credentialName].token }}"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "bodyContentType": "json",
        "jsonBody": "={{ JSON.stringify($('Create Processing Batches').item.json.batch) }}",
        "options": {
          "timeout": "={{ parseInt($env.PUBLISH_TIMEOUT) || 120000 }}",
          "retry": {
            "enabled": true,
            "maxTries": "={{ parseInt($env.PUBLISH_MAX_RETRIES) || 5 }}",
            "waitBetween": "={{ parseInt($env.PUBLISH_RETRY_DELAY) || 3000 }}"
          }
        }
      },
      "name": "Publish Documents",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [2000, 150],
      "id": "publishDocs1",
      "notes": "Publish approved documents using configurable timeouts\nExtended timeout and retry logic for large document batches\nTenant-specific credential resolution"
    },
    {
      "parameters": {
        "functionCode": "// Log policy-blocked documents with tenant context\nconst previewResult = $json;\nconst batchInfo = $('Create Processing Batches').item.json;\n\nconst logData = {\n  status: 'blocked',\n  tenant: batchInfo.tenantId,\n  batchNumber: batchInfo.batchNumber,\n  totalBatches: batchInfo.totalBatches,\n  batchSize: batchInfo.batchSize,\n  newDocs: batchInfo.newDocs,\n  modifiedDocs: batchInfo.modifiedDocs,\n  wouldPublish: previewResult.wouldPublish,\n  findings: previewResult.findings || [],\n  errors: previewResult.errors || [],\n  timestamp: new Date().toISOString()\n};\n\nconsole.log(`BLOCKED: ${batchInfo.tenantId} batch ${batchInfo.batchNumber}/${batchInfo.totalBatches}`);\nconsole.log(`- Documents: ${batchInfo.batchSize} (${batchInfo.newDocs} new, ${batchInfo.modifiedDocs} modified)`);\nconsole.log(`- Would publish: ${previewResult.wouldPublish}`);\nconsole.log(`- PII findings: ${JSON.stringify(previewResult.findings)}`);\n\nif (previewResult.errors && previewResult.errors.length > 0) {\n  console.log(`- Errors: ${previewResult.errors.join(', ')}`);\n}\n\nreturn [logData];"
      },
      "name": "Log Policy Block",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [2000, 350],
      "id": "logBlock1",
      "notes": "Log policy-blocked documents with tenant-specific context\nProvides detailed audit trail for compliance monitoring"
    },
    {
      "parameters": {
        "functionCode": "// Log successful publication with comprehensive tenant metrics\nconst publishResult = $json;\nconst batchInfo = $('Create Processing Batches').item.json;\n\nconst logData = {\n  status: 'published',\n  tenant: batchInfo.tenantId,\n  batchNumber: batchInfo.batchNumber,\n  totalBatches: batchInfo.totalBatches,\n  batchSize: batchInfo.batchSize,\n  newDocs: batchInfo.newDocs,\n  modifiedDocs: batchInfo.modifiedDocs,\n  summary: publishResult.summary || {},\n  results: publishResult.results || [],\n  timestamp: new Date().toISOString()\n};\n\nconsole.log(`PUBLISHED: ${batchInfo.tenantId} batch ${batchInfo.batchNumber}/${batchInfo.totalBatches}`);\nconsole.log(`- Input: ${batchInfo.batchSize} docs (${batchInfo.newDocs} new, ${batchInfo.modifiedDocs} modified)`);\n\nif (publishResult.summary) {\n  const s = publishResult.summary;\n  console.log(`- Published: ${s.published}, Updated: ${s.updated}, Blocked: ${s.blocked}, Errors: ${s.errors}`);\n}\n\n// Log individual document results\nif (publishResult.results) {\n  publishResult.results.forEach(result => {\n    const status = result.status;\n    const points = result.pointsUpserted ? ` (${result.pointsUpserted} points)` : '';\n    console.log(`  - ${result.docId}: ${status}${points}`);\n    if (result.message) {\n      console.log(`    ${result.message}`);\n    }\n  });\n}\n\nreturn [logData];"
      },
      "name": "Log Publish Success",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [2200, 150],
      "id": "logSuccess1",
      "notes": "Comprehensive success logging with tenant-specific metrics\nTracks both new and modified document processing results"
    },
    {
      "parameters": {
        "functionCode": "// Configurable tombstone detection and handling\nconst initData = $('Initialize Configuration').item.json;\nconst currentFiles = $workflow.staticData.currentFiles || [];\nconst previousFiles = initData.previousFiles || [];\nconst config = initData.config;\n\n// Skip tombstone processing if disabled\nif (!config.enableTombstones) {\n  console.log('Tombstone processing disabled');\n  return [];\n}\n\n// Find deleted files\nconst deletedFiles = previousFiles.filter(file => !currentFiles.includes(file));\n\nif (deletedFiles.length === 0) {\n  console.log('No deleted files detected');\n  return [];\n}\n\nconsole.log(`TOMBSTONES: Detected ${deletedFiles.length} deleted files for ${config.tenantId}`);\n\n// Create tombstone documents\nconst tombstones = deletedFiles.map(filePath => {\n  const path = require('path');\n  const relativePath = path.relative(config.obsidianVaultPath, filePath);\n  const docId = relativePath.replace(/\\.(md|markdown)$/i, '').replace(/\\\\/g, '/');\n  \n  console.log(`- Creating tombstone for: ${docId}`);\n  \n  return {\n    meta: {\n      tenant: config.tenantId,\n      docId: docId,\n      source: config.source,\n      path: relativePath.replace(/\\\\/g, '/'),\n      title: `[DELETED] ${path.basename(filePath, path.extname(filePath))}`,\n      lang: config.lang,\n      version: '1.0',\n      sha256: 'tombstone',\n      acl: ['system'],\n      authors: [],\n      tags: ['deleted', 'tombstone'],\n      timestamp: new Date().toISOString(),\n      modifiedAt: new Date().toISOString(),\n      deleted: true\n    },\n    blocks: []\n  };\n});\n\nreturn [{\n  tombstones: tombstones,\n  deletedCount: deletedFiles.length,\n  deletedFiles: deletedFiles,\n  tenantId: config.tenantId\n}];"
      },
      "name": "Detect File Deletions",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [1400, 450],
      "id": "detectDeletions1",
      "notes": "Configurable deletion detection with tombstone creation\nSkips processing when ENABLE_TOMBSTONES=false\nTracks file paths for proper vector cleanup"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "has-deletions",
              "leftValue": "={{ $json.deletedCount }}",
              "rightValue": 0,
              "operator": {
                "type": "number",
                "operation": "gt"
              }
            }
          ],
          "combinator": "and"
        }
      },
      "name": "Check for Deletions",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [1600, 450],
      "id": "checkDeletions1",
      "notes": "Conditional tombstone processing\nOnly publishes tombstones when deletions are detected"
    },
    {
      "parameters": {
        "url": "={{ $env.API_URL }}/ingest/publish",
        "method": "POST",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "x-ingest-token",
              "value": "={{ $credentials[$('Initialize Configuration').item.json.config.credentialName].token }}"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "bodyContentType": "json",
        "jsonBody": "={{ JSON.stringify($json.tombstones) }}",
        "options": {
          "timeout": "={{ parseInt($env.TOMBSTONE_TIMEOUT) || 60000 }}",
          "retry": {
            "enabled": true,
            "maxTries": "={{ parseInt($env.TOMBSTONE_MAX_RETRIES) || 5 }}",
            "waitBetween": "={{ parseInt($env.TOMBSTONE_RETRY_DELAY) || 2000 }}"
          }
        }
      },
      "name": "Publish Tombstones",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [1800, 400],
      "id": "publishTombstones1",
      "notes": "Publish tombstone documents with configurable timeouts\nUses tenant-specific credentials for deletion processing\nEnsures proper vector cleanup in search index"
    },
    {
      "parameters": {
        "functionCode": "// Update workflow state with comprehensive persistence\nconst initData = $('Initialize Configuration').item.json;\nconst currentFiles = $workflow.staticData.currentFiles || [];\nconst updatedFileHashes = $workflow.staticData.updatedFileHashes || {};\nconst config = initData.config;\n\n// Clean up file hashes for deleted files to prevent memory leaks\nconst cleanedFileHashes = {};\nif (config.useIdempotency) {\n  for (const [filePath, hash] of Object.entries(updatedFileHashes)) {\n    if (currentFiles.includes(filePath)) {\n      cleanedFileHashes[filePath] = hash;\n    }\n  }\n}\n\n// Calculate processing statistics\nconst processedFiles = $('Apply Idempotency Logic').all().length;\nconst skippedFiles = currentFiles.length - processedFiles;\n\n// Update workflow static data\nconst updatedStaticData = {\n  lastRunTime: initData.currentRunTime,\n  previousFiles: config.enableTombstones ? currentFiles : [],\n  fileHashes: cleanedFileHashes,\n  lastRunSummary: {\n    timestamp: initData.currentRunTime,\n    tenant: config.tenantId,\n    filesProcessed: processedFiles,\n    filesSkipped: skippedFiles,\n    totalFiles: currentFiles.length,\n    idempotencyEnabled: config.useIdempotency,\n    tombstonesEnabled: config.enableTombstones\n  }\n};\n\n$workflow.staticData = updatedStaticData;\n\nconsole.log(`STATE UPDATE: ${config.tenantId} workflow state updated`);\nconsole.log(`- Files tracked: ${currentFiles.length}`);\nconsole.log(`- File hashes: ${Object.keys(cleanedFileHashes).length}`);\nconsole.log(`- Files processed: ${processedFiles}`);\nconsole.log(`- Files skipped: ${skippedFiles}`);\n\nreturn [{\n  status: 'state_updated',\n  tenant: config.tenantId,\n  timestamp: initData.currentRunTime,\n  summary: updatedStaticData.lastRunSummary\n}];"
      },
      "name": "Update Workflow State",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [2400, 250],
      "id": "updateState1",
      "notes": "Persist workflow state with tenant-specific tracking\nCleans up orphaned file hashes to prevent memory growth\nConfigurable state management based on feature flags"
    },
    {
      "parameters": {
        "functionCode": "// Comprehensive workflow execution summary with tenant metrics\nconst publishResults = $('Log Publish Success').all();\nconst blockResults = $('Log Policy Block').all();\nconst tombstoneResult = $('Publish Tombstones').first();\nconst stateUpdate = $('Update Workflow State').first();\nconst config = $('Initialize Configuration').item.json.config;\n\nconst summary = {\n  workflow: 'obsidian-sync-template',\n  tenant: config.tenantId,\n  timestamp: new Date().toISOString(),\n  configuration: {\n    vaultPath: config.obsidianVaultPath,\n    batchSize: config.batchSize,\n    incrementalSync: config.incrementalSync,\n    idempotencyEnabled: config.useIdempotency,\n    tombstonesEnabled: config.enableTombstones\n  },\n  execution: {\n    totalBatches: publishResults.length + blockResults.length,\n    publishedBatches: publishResults.length,\n    blockedBatches: blockResults.length\n  },\n  documents: {\n    total: 0,\n    published: 0,\n    updated: 0,\n    blocked: 0,\n    errors: 0,\n    skipped: stateUpdate?.json?.summary?.filesSkipped || 0\n  },\n  tombstones: {\n    created: tombstoneResult?.json ? $('Detect File Deletions').first()?.json?.deletedCount || 0 : 0,\n    published: tombstoneResult ? 1 : 0\n  },\n  state: stateUpdate?.json?.summary || {}\n};\n\n// Aggregate document statistics\npublishResults.forEach(result => {\n  if (result.json.summary) {\n    const s = result.json.summary;\n    summary.documents.total += s.total || 0;\n    summary.documents.published += s.published || 0;\n    summary.documents.updated += s.updated || 0;\n    summary.documents.blocked += s.blocked || 0;\n    summary.documents.errors += s.errors || 0;\n  }\n});\n\nblockResults.forEach(result => {\n  summary.documents.blocked += result.json.batchSize || 0;\n});\n\n// Generate tenant-specific report\nconsole.log('=====================================');\nconsole.log(`OBSIDIAN SYNC COMPLETE: ${config.tenantId.toUpperCase()}`);\nconsole.log('=====================================');\nconsole.log(`Vault: ${config.obsidianVaultPath}`);\nconsole.log(`Timestamp: ${summary.timestamp}`);\nconsole.log(`Configuration: batch=${config.batchSize}, incremental=${config.incrementalSync}, idempotency=${config.useIdempotency}, tombstones=${config.enableTombstones}`);\nconsole.log(`Batches: ${summary.execution.publishedBatches} published, ${summary.execution.blockedBatches} blocked`);\nconsole.log(`Documents: ${summary.documents.published} published, ${summary.documents.updated} updated, ${summary.documents.blocked} blocked, ${summary.documents.errors} errors, ${summary.documents.skipped} skipped`);\nconsole.log(`Tombstones: ${summary.tombstones.created} created`);\nconsole.log(`Files tracked: ${summary.state.totalFiles}`);\nconsole.log('=====================================');\n\nreturn [summary];"
      },
      "name": "Final Execution Summary",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [2600, 250],
      "id": "finalSummary1",
      "notes": "Comprehensive execution summary with tenant-specific metrics\nProvides detailed configuration and performance data\nConfigurable reporting based on enabled features"
    },
    {
      "parameters": {
        "functionCode": "// Enhanced error handler with tenant context and alerting hooks\nconst error = $json.error || $json;\nconst timestamp = new Date().toISOString();\nconst config = $('Initialize Configuration').item?.json?.config || { tenantId: 'unknown' };\n\nconst errorData = {\n  workflow: 'obsidian-sync-template',\n  tenant: config.tenantId,\n  timestamp,\n  error: {\n    message: error.message || JSON.stringify(error),\n    stack: error.stack || 'No stack trace available',\n    node: $node?.name || 'Unknown node',\n    context: {\n      vaultPath: config.obsidianVaultPath || 'unknown',\n      lastSuccessfulRun: $workflow.staticData?.lastRunTime || 'Never',\n      filesBeingProcessed: $workflow.staticData?.currentFiles?.length || 0,\n      configuration: {\n        idempotencyEnabled: config.useIdempotency,\n        tombstonesEnabled: config.enableTombstones,\n        batchSize: config.batchSize\n      }\n    }\n  }\n};\n\nconsole.error('==========================================');\nconsole.error(`OBSIDIAN SYNC ERROR: ${config.tenantId.toUpperCase()}`);\nconsole.error('==========================================');\nconsole.error(`Time: ${timestamp}`);\nconsole.error(`Vault: ${config.obsidianVaultPath}`);\nconsole.error(`Node: ${errorData.error.node}`);\nconsole.error(`Error: ${errorData.error.message}`);\nconsole.error(`Last successful run: ${errorData.error.context.lastSuccessfulRun}`);\nconsole.error('==========================================');\n\n// Store error in static data for monitoring\nconst staticData = $workflow.staticData || {};\nstaticData.lastError = errorData;\n$workflow.staticData = staticData;\n\n// TODO: Add configurable alerting integration\n// Example environment variables for alerting:\n// - ALERT_WEBHOOK_URL\n// - SLACK_WEBHOOK_URL\n// - EMAIL_ALERT_ENDPOINT\n// - TEAMS_WEBHOOK_URL\n\nreturn [errorData];"
      },
      "name": "Enhanced Error Handler",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [2600, 450],
      "id": "errorHandler1",
      "notes": "Enhanced error handling with tenant context\nStores comprehensive error state for monitoring\nProvides hooks for configurable alerting integrations"
    }
  ],
  "connections": {
    "Manual Trigger": {
      "main": [
        [
          {
            "node": "Initialize Configuration",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Cron Trigger": {
      "main": [
        [
          {
            "node": "Initialize Configuration",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Initialize Configuration": {
      "main": [
        [
          {
            "node": "Build File Discovery Command",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build File Discovery Command": {
      "main": [
        [
          {
            "node": "Find Markdown Files",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Find Markdown Files": {
      "main": [
        [
          {
            "node": "Apply Idempotency Logic",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Apply Idempotency Logic": {
      "main": [
        [
          {
            "node": "Parse Markdown Documents",
            "type": "main",
            "index": 0
          },
          {
            "node": "Detect File Deletions",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Markdown Documents": {
      "main": [
        [
          {
            "node": "Create Processing Batches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create Processing Batches": {
      "main": [
        [
          {
            "node": "Preview Documents",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Preview Documents": {
      "main": [
        [
          {
            "node": "Check Preview Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check Preview Result": {
      "main": [
        [
          {
            "node": "Publish Documents",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Log Policy Block",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Publish Documents": {
      "main": [
        [
          {
            "node": "Log Publish Success",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log Policy Block": {
      "main": [
        [
          {
            "node": "Update Workflow State",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log Publish Success": {
      "main": [
        [
          {
            "node": "Update Workflow State",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Detect File Deletions": {
      "main": [
        [
          {
            "node": "Check for Deletions",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check for Deletions": {
      "main": [
        [
          {
            "node": "Publish Tombstones",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Update Workflow State",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Publish Tombstones": {
      "main": [
        [
          {
            "node": "Update Workflow State",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Update Workflow State": {
      "main": [
        [
          {
            "node": "Final Execution Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "pinData": {},
  "settings": {
    "executionOrder": "v1",
    "saveManualExecutions": true,
    "callerPolicy": "workflowsFromSameOwner",
    "errorWorkflow": "Enhanced Error Handler"
  },
  "staticData": {},
  "tags": [
    {
      "createdAt": "2024-01-01T00:00:00.000Z",
      "updatedAt": "2024-01-01T00:00:00.000Z",
      "id": "template",
      "name": "template"
    },
    {
      "createdAt": "2024-01-01T00:00:00.000Z",
      "updatedAt": "2024-01-01T00:00:00.000Z",
      "id": "obsidian",
      "name": "obsidian"
    },
    {
      "createdAt": "2024-01-01T00:00:00.000Z",
      "updatedAt": "2024-01-01T00:00:00.000Z",
      "id": "ingestion",
      "name": "ingestion"
    }
  ],
  "triggerCount": 2,
  "updatedAt": "2024-01-01T00:00:00.000Z",
  "versionId": "obsidian-sync-template-v2.0",
  "name": "Obsidian Markdown Sync Template",
  "active": false,
  "id": "obsidian-sync-template",
  "createdAt": "2024-01-01T00:00:00.000Z"
}