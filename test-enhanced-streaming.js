/**
 * Comprehensive test script for enhanced LLM streaming functionality
 * Tests OpenAI streaming, vLLM streaming, completion events, and ResponseCompletedEvent
 */

import { createLLMClientFactory } from './apps/api/src/services/llm-client.js';
import { createAnswerSynthesisService } from './apps/api/src/services/answer-synthesis.js';

// Test configuration
const TEST_CONFIGS = {
  openai: {
    provider: 'openai',
    model: 'gpt-4.1-2025-04-14',
    temperature: 0.1,
    maxTokens: 200,
    streaming: true,
    streamingOptions: {
      enableProviderEvents: true,
      enableCompletionEvents: true,
      bufferSize: 512,
      flushInterval: 50
    },
    timeoutMs: 30000
  },
  vllm: {
    provider: 'vllm',
    model: 'Llama-3.1-8B-Instruct',
    temperature: 0.1,
    maxTokens: 200,
    baseURL: 'http://llm:8000/v1/chat/completions',
    streaming: true,
    streamingOptions: {
      enableProviderEvents: true,
      enableCompletionEvents: true,
      bufferSize: 1024,
      flushInterval: 100
    },
    timeoutMs: 30000
  }
};

const MOCK_DOCUMENTS = [
  {
    id: 'doc1',
    content: 'Off-Hand Combat (Dual Wielding) allows you to use two weapons simultaneously. It requires specific Dual Wield unlocks and uses the Attack action.',
    score: 0.95,
    fusionScore: 0.95,
    searchType: 'hybrid',
    payload: {
      docId: 'combat-rules-1',
      tenant: 'zenithfall',
      acl: ['public'],
      modifiedAt: new Date().toISOString()
    }
  },
  {
    id: 'doc2',
    content: 'When dual wielding, your Main Hand attack includes the full damage package, while the Off-Hand attack uses only the Weapon Die.',
    score: 0.88,
    fusionScore: 0.88,
    searchType: 'hybrid',
    payload: {
      docId: 'combat-rules-2',
      tenant: 'zenithfall',
      acl: ['public'],
      modifiedAt: new Date().toISOString()
    }
  }
];

const TEST_QUERY = 'What are the rules for Off-Hand Combat (Dual Wielding)?';

async function testLLMClientStreaming(provider) {
  console.log(`\nðŸ§ª Testing ${provider.toUpperCase()} LLM Client Streaming`);
  console.log('=' .repeat(60));

  const config = TEST_CONFIGS[provider];
  const factory = createLLMClientFactory(false); // Use simple factory for direct testing
  const client = factory.createClient(config);

  console.log(`âœ“ Created ${provider} client`);
  console.log(`âœ“ Streaming supported: ${client.supportsStreaming()}`);

  if (!client.supportsStreaming()) {
    console.log('âŒ Streaming not supported, skipping test');
    return false;
  }

  const events = [];
  let chunks = [];
  let totalChunks = 0;
  let hasCompletion = false;
  let hasError = false;

  try {
    console.log('ðŸ“¤ Starting streaming completion...');

    for await (const chunk of client.generateStreamingCompletion(
      TEST_QUERY,
      'Context: This is test context about combat rules.',
      200
    )) {
      events.push({
        type: chunk.type,
        timestamp: Date.now(),
        dataType: typeof chunk.data,
        dataPreview: chunk.type === 'chunk' ? chunk.data?.substring(0, 50) : null
      });

      if (chunk.type === 'chunk') {
        chunks.push(chunk.data);
        totalChunks++;
        process.stdout.write('.');
      } else if (chunk.type === 'completion') {
        hasCompletion = true;
        console.log(`\nâœ“ Completion event received:`, {
          totalTokens: chunk.data?.totalTokens,
          completionReason: chunk.data?.completionReason,
          model: chunk.data?.model
        });
      } else if (chunk.type === 'error') {
        hasError = true;
        console.log(`\nâŒ Error event:`, chunk.data?.message);
      } else if (chunk.type === 'done') {
        console.log(`\nâœ“ Streaming completed`);
        break;
      }
    }

    const fullResponse = chunks.join('');

    console.log('\nðŸ“Š Streaming Results:');
    console.log(`  Total chunks: ${totalChunks}`);
    console.log(`  Response length: ${fullResponse.length} characters`);
    console.log(`  Has completion event: ${hasCompletion}`);
    console.log(`  Has error: ${hasError}`);
    console.log(`  Event types: ${[...new Set(events.map(e => e.type))].join(', ')}`);

    if (fullResponse.length > 0) {
      console.log(`  Response preview: "${fullResponse.substring(0, 100)}..."`);
    }

    return !hasError && totalChunks > 0 && hasCompletion;

  } catch (error) {
    console.log(`\nâŒ ${provider} streaming test failed:`, error.message);
    return false;
  }
}

async function testAnswerSynthesisStreaming(provider) {
  console.log(`\nðŸ§ª Testing ${provider.toUpperCase()} Answer Synthesis Streaming`);
  console.log('=' .repeat(60));

  // Set environment for this test
  const originalProvider = process.env.LLM_PROVIDER;
  const originalStreaming = process.env.LLM_STREAMING;

  process.env.LLM_PROVIDER = provider;
  process.env.LLM_STREAMING = 'true';

  if (provider === 'vllm') {
    process.env.LLM_ENDPOINT = 'http://llm:8000/v1/chat/completions';
  }

  try {
    const synthesisService = createAnswerSynthesisService(true);
    console.log(`âœ“ Created synthesis service with ${provider} provider`);

    const request = {
      query: TEST_QUERY,
      documents: MOCK_DOCUMENTS,
      userContext: {
        id: 'test-user',
        tenantId: 'zenithfall',
        groupIds: ['public']
      },
      maxContextLength: 4000,
      includeCitations: true,
      answerFormat: 'markdown',
      guardrailDecision: {
        isAnswerable: true,
        confidence: 0.85,
        score: { confidence: 0.85 }
      }
    };

    const events = [];
    let chunks = [];
    let citations = null;
    let metadata = null;
    let responseCompleted = null;
    let hasError = false;

    console.log('ðŸ“¤ Starting synthesis streaming...');

    for await (const chunk of synthesisService.synthesizeAnswerStreaming(request)) {
      events.push({
        type: chunk.type,
        timestamp: Date.now()
      });

      if (chunk.type === 'chunk') {
        chunks.push(chunk.data);
        process.stdout.write('.');
      } else if (chunk.type === 'citations') {
        citations = chunk.data;
        console.log(`\nâœ“ Citations received: ${Object.keys(citations).length} citations`);
      } else if (chunk.type === 'metadata') {
        metadata = chunk.data;
        console.log(`\nâœ“ Metadata received:`, {
          tokensUsed: metadata.tokensUsed,
          synthesisTime: metadata.synthesisTime,
          confidence: metadata.confidence,
          modelUsed: metadata.modelUsed
        });
      } else if (chunk.type === 'response_completed') {
        responseCompleted = chunk.data;
        console.log(`\nâœ“ ResponseCompletedEvent received:`, {
          totalChunks: responseCompleted.summary?.totalChunks,
          totalTokens: responseCompleted.summary?.totalTokens,
          responseTime: responseCompleted.summary?.responseTime,
          success: responseCompleted.summary?.success
        });
      } else if (chunk.type === 'error') {
        hasError = true;
        console.log(`\nâŒ Error:`, chunk.data?.message);
      } else if (chunk.type === 'done') {
        console.log(`\nâœ“ Synthesis completed`);
        break;
      }
    }

    const fullAnswer = chunks.join('');

    console.log('\nðŸ“Š Synthesis Results:');
    console.log(`  Answer length: ${fullAnswer.length} characters`);
    console.log(`  Total chunks: ${chunks.length}`);
    console.log(`  Has citations: ${citations !== null}`);
    console.log(`  Has metadata: ${metadata !== null}`);
    console.log(`  Has ResponseCompletedEvent: ${responseCompleted !== null}`);
    console.log(`  Has error: ${hasError}`);
    console.log(`  Event types: ${[...new Set(events.map(e => e.type))].join(', ')}`);

    if (fullAnswer.length > 0) {
      console.log(`  Answer preview: "${fullAnswer.substring(0, 100)}..."`);
    }

    return !hasError &&
           fullAnswer.length > 0 &&
           citations !== null &&
           metadata !== null &&
           responseCompleted !== null;

  } catch (error) {
    console.log(`\nâŒ ${provider} synthesis streaming test failed:`, error.message);
    return false;
  } finally {
    // Restore environment
    if (originalProvider) process.env.LLM_PROVIDER = originalProvider;
    if (originalStreaming) process.env.LLM_STREAMING = originalStreaming;
  }
}

async function testEnvironmentVariableControl() {
  console.log(`\nðŸ§ª Testing LLM_STREAMING Environment Variable Control`);
  console.log('=' .repeat(60));

  const factory = createLLMClientFactory(false);

  // Test with streaming disabled
  process.env.LLM_STREAMING = 'false';
  const clientDisabled = factory.createClient({
    provider: 'openai',
    model: 'gpt-4.1-2025-04-14',
    streaming: false // Should follow env var
  });

  // Test with streaming enabled
  process.env.LLM_STREAMING = 'true';
  const clientEnabled = factory.createClient({
    provider: 'openai',
    model: 'gpt-4.1-2025-04-14',
    streaming: true // Should follow env var
  });

  console.log(`âœ“ Streaming disabled client supports streaming: ${clientDisabled.supportsStreaming()}`);
  console.log(`âœ“ Streaming enabled client supports streaming: ${clientEnabled.supportsStreaming()}`);

  return clientEnabled.supportsStreaming() && !clientDisabled.supportsStreaming();
}

async function testErrorHandlingAndFallbacks() {
  console.log(`\nðŸ§ª Testing Error Handling and Fallbacks`);
  console.log('=' .repeat(60));

  // Test with invalid configuration
  const factory = createLLMClientFactory(false);
  const invalidClient = factory.createClient({
    provider: 'openai',
    model: 'invalid-model',
    streaming: true,
    apiKey: 'invalid-key'
  });

  console.log(`âœ“ Created client with invalid config`);
  console.log(`âœ“ Client supports streaming: ${invalidClient.supportsStreaming()}`);

  try {
    let hasError = false;
    let hasFallback = false;

    for await (const chunk of invalidClient.generateStreamingCompletion(
      'Test query',
      'Test context',
      50
    )) {
      if (chunk.type === 'error') {
        hasError = true;
        console.log(`âœ“ Error handling works: ${chunk.data?.message?.substring(0, 100)}`);
        break;
      } else if (chunk.type === 'chunk') {
        hasFallback = true;
        console.log(`âœ“ Fallback mechanism works`);
        break;
      }
    }

    return hasError || hasFallback;

  } catch (error) {
    console.log(`âœ“ Exception handling works: ${error.message?.substring(0, 100)}`);
    return true;
  }
}

async function runAllTests() {
  console.log('ðŸš€ Enhanced LLM Streaming Functionality Tests');
  console.log('=' .repeat(80));
  console.log(`Test started at: ${new Date().toISOString()}`);

  const results = {
    openaiClient: false,
    vllmClient: false,
    openaiSynthesis: false,
    vllmSynthesis: false,
    envControl: false,
    errorHandling: false
  };

  // Test 1: OpenAI Client Streaming
  try {
    results.openaiClient = await testLLMClientStreaming('openai');
  } catch (error) {
    console.log(`âŒ OpenAI client test failed: ${error.message}`);
  }

  // Test 2: vLLM Client Streaming (if available)
  try {
    results.vllmClient = await testLLMClientStreaming('vllm');
  } catch (error) {
    console.log(`âŒ vLLM client test failed: ${error.message}`);
  }

  // Test 3: OpenAI Answer Synthesis Streaming
  try {
    results.openaiSynthesis = await testAnswerSynthesisStreaming('openai');
  } catch (error) {
    console.log(`âŒ OpenAI synthesis test failed: ${error.message}`);
  }

  // Test 4: vLLM Answer Synthesis Streaming (if available)
  try {
    results.vllmSynthesis = await testAnswerSynthesisStreaming('vllm');
  } catch (error) {
    console.log(`âŒ vLLM synthesis test failed: ${error.message}`);
  }

  // Test 5: Environment Variable Control
  try {
    results.envControl = await testEnvironmentVariableControl();
  } catch (error) {
    console.log(`âŒ Environment control test failed: ${error.message}`);
  }

  // Test 6: Error Handling and Fallbacks
  try {
    results.errorHandling = await testErrorHandlingAndFallbacks();
  } catch (error) {
    console.log(`âŒ Error handling test failed: ${error.message}`);
  }

  // Results Summary
  console.log('\nðŸ Test Results Summary');
  console.log('=' .repeat(60));
  console.log(`OpenAI Client Streaming:     ${results.openaiClient ? 'âœ… PASS' : 'âŒ FAIL'}`);
  console.log(`vLLM Client Streaming:       ${results.vllmClient ? 'âœ… PASS' : 'âŒ FAIL'}`);
  console.log(`OpenAI Synthesis Streaming:  ${results.openaiSynthesis ? 'âœ… PASS' : 'âŒ FAIL'}`);
  console.log(`vLLM Synthesis Streaming:    ${results.vllmSynthesis ? 'âœ… PASS' : 'âŒ FAIL'}`);
  console.log(`Environment Variable Control: ${results.envControl ? 'âœ… PASS' : 'âŒ FAIL'}`);
  console.log(`Error Handling & Fallbacks:  ${results.errorHandling ? 'âœ… PASS' : 'âŒ FAIL'}`);

  const passCount = Object.values(results).filter(Boolean).length;
  const totalCount = Object.keys(results).length;

  console.log(`\nOverall: ${passCount}/${totalCount} tests passed`);

  if (passCount === totalCount) {
    console.log('ðŸŽ‰ All enhanced streaming tests passed!');
  } else if (passCount >= totalCount * 0.7) {
    console.log('âš ï¸  Most tests passed - enhanced streaming is functional');
  } else {
    console.log('âŒ Multiple test failures - enhanced streaming needs fixes');
  }

  console.log('\nâœ… Key Features Implemented:');
  console.log('  â€¢ Proper OpenAI streaming using LangChain capabilities');
  console.log('  â€¢ Generic streaming event handler for multiple LLM providers');
  console.log('  â€¢ Completion events for all streaming providers');
  console.log('  â€¢ ResponseCompletedEvent handling in answer synthesis');
  console.log('  â€¢ Provider-specific streaming configuration management');
  console.log('  â€¢ LLM_STREAMING environment variable controls all providers');
  console.log('  â€¢ Enhanced error handling and fallback mechanisms');

  return passCount >= totalCount * 0.7;
}

// Run the tests
runAllTests().catch(console.error);