# Instance Configuration - Change these for each client/environment
INSTANCE_NAME=demo
PROJECT_PREFIX=cw-rag

# Tenant Configuration
TENANT=zenithfall

# Vector Configuration
VECTOR_DIM=384

# PII Policy Configuration
PII_POLICY=OFF

# Embeddings Configuration
EMBEDDINGS_PROVIDER=local
EMBEDDINGS_MODEL=bge-small-en-v1.5

# Port Configuration - Adjust for parallel instances
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334
API_PORT=3000
WEB_PORT=3001
N8N_PORT=5678

# Qdrant Configuration
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION=docs_v1

# API Configuration
API_URL=http://localhost:3000
CORS_ORIGIN=http://localhost:3001

# Security Configuration
INGEST_TOKEN=your-secure-random-token-here-change-this-in-production

# Web Configuration
NEXT_PUBLIC_API_URL=http://localhost:3000

# N8N Configuration (if using external instance)
N8N_HOST=localhost

# Docker Configuration
COMPOSE_PROJECT_NAME=${PROJECT_PREFIX}-${INSTANCE_NAME}
QDRANT_CONTAINER_NAME=${PROJECT_PREFIX}-${INSTANCE_NAME}-qdrant
API_CONTAINER_NAME=${PROJECT_PREFIX}-${INSTANCE_NAME}-api
WEB_CONTAINER_NAME=${PROJECT_PREFIX}-${INSTANCE_NAME}-web

# Volume Configuration
QDRANT_VOLUME_NAME=${PROJECT_PREFIX}_${INSTANCE_NAME}_qdrant_storage

# Network Configuration
NETWORK_NAME=${PROJECT_PREFIX}-${INSTANCE_NAME}-network

# LLM Configuration - OpenAI Default (vLLM available for future clients)
LLM_ENABLED=true
LLM_PROVIDER=openai
LLM_MODEL=gpt-4
LLM_STREAMING=false
LLM_TIMEOUT_MS=25000

# OpenAI API Configuration
OPENAI_API_KEY=your-openai-api-key-here

# vLLM Configuration (for future clients - uncomment to use)
# LLM_PROVIDER=vllm
# LLM_MODEL=Llama-3.1-8B-Instruct
# LLM_ENDPOINT=http://llm:8000/v1/chat/completions
# LLM_STREAMING=true

# Answerability Guardrail Configuration
ANSWERABILITY_THRESHOLD=0.5